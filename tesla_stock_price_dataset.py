# -*- coding: utf-8 -*-
"""Tesla Stock Price Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ch9VsvPKR5FyEY2OZvQZ7Vm3fmPYh2ne

*                   Tesla Stock Price Prediction*

Import Libraries :

First, import all the required libraries:
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

"""Load the Dataset

"""

df= pd.read_csv('/content/TSLA.csv')
df

df.head()

df.tail()

df.info()

df.describe().T

"""### Data Visualization"""

df.describe().T.plot(kind='bar')

df.describe(include='all').T

"""Plot the Closing Price over Time"""

fig, ax = plt.subplots(figsize=(20, 8))
ax.plot(df['Date'], df['Close'], color='green')
ax.xaxis.set_major_locator(plt.MaxNLocator(15))
ax.set_xlabel('Date', fontsize=14)
ax.set_ylabel('Price in USD', fontsize=14)
plt.title('Tesla Stock Price Dataset  ', fontsize=18)
plt.grid()
plt.show()

"""Bar Plot of Closing Prices"""

# Bar plot
fig2, ax = plt.subplots(figsize=(20, 8))
ax.bar(df['Date'], df['Close'], color='green')
ax.xaxis.set_major_locator(plt.MaxNLocator(15))
ax.set_xlabel('Date', fontsize=14)
ax.set_ylabel('Price in USD', fontsize=14)
plt.title('Tesla Stock Price Dataset', fontsize=18)
plt.grid()
plt.show()

"""Histogram of All Numeric Columns"""

df.hist(bins = 20, figsize = (20,20), color = 'g')
plt.show()

df.columns.to_list()

"""Plotly Express Histograms"""

from plotly import express

for column in ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']:
    express.histogram(data_frame=df, x=column).show()

"""Correlation Matrix Heatmap"""

numeric_cols = df.select_dtypes(include=np.number).columns  # اختيار الأعمدة الرقمية فقط
plt.figure(figsize=(12, 8))
sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""Train a Random Forest Model"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

features = ["Open", "High", "Low", "Volume"]
#select the elements
X = df[features]
y = df["Close"]

#the goal
X_train, X_test, y_train, y_test = train_test_split(X, y)
model = RandomForestRegressor()

model.fit(X_train, y_train)

from sklearn.metrics import mean_squared_error

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate the mean squared error
mse = mean_squared_error(y_test, y_pred)

print("Mean Squared Error:", mse)

"""Plot Actual vs. Predicted Closing Prices"""

prediction_y = model.predict(X_test)#prediction output

dates =  df.iloc[X_test.index]["Date"]

prediction_df = pd.DataFrame({"Date": dates, "Predicted Closing Price": prediction_y})
prediction_df.sort_values("Date", inplace=True)

#draw the graph
plt.figure(figsize = (30,15))
plt.plot(df["Date"], df["Close"], label = "Actual Closing Price")
plt.plot(prediction_df["Date"], prediction_df["Predicted Closing Price"], label = "Predicted Closing Price")
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title("Actual vs. Predicted Closing Prices")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.datasets import make_classification

# Load the dataset
df = pd.read_csv('/content/TSLA.csv')

# For demonstration purposes, we need a classification dataset.
# Since the Tesla dataset is a regression problem, we'll use a synthetic dataset for classification.
X, y = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=0, random_state=42)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Implementing the bootstrapping algorithm
def bootstrap_sample(X, y):
    n_samples = X.shape[0]
    indices = np.random.choice(n_samples, size=n_samples, replace=True)
    return X[indices], y[indices]

# Training the Random Forest Classifier with bootstrapping
n_trees = 100
forest = []

for _ in range(n_trees):
    X_sample, y_sample = bootstrap_sample(X_train, y_train)
    tree = RandomForestClassifier(n_estimators=1, bootstrap=False)
    tree.fit(X_sample, y_sample)
    forest.append(tree)

# Making predictions
def forest_predict(forest, X):
    tree_preds = np.array([tree.predict(X) for tree in forest])
    return np.squeeze(np.apply_along_axis(lambda x: np.bincount(x, minlength=2).argmax(), arr=tree_preds, axis=0))

y_pred = forest_predict(forest, X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:\n", classification_report(y_test, y_pred))

# Plotting the first two features of the dataset for visualization
plt.figure(figsize=(12, 8))
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='coolwarm', alpha=0.6, edgecolor='w', s=100)
plt.title("Random Forest Classifier Predictions")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

"""Decision Tree Classifier

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.datasets import make_classification

# For demonstration purposes, we need a classification dataset.
# Since the Tesla dataset is a regression problem, we'll use a synthetic dataset for classification.
X, y = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=0, random_state=42)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Training the Decision Tree Classifier
tree = DecisionTreeClassifier(random_state=42)
tree.fit(X_train, y_train)

# Making predictions
y_pred = tree.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:\n", classification_report(y_test, y_pred))

# Plotting the first two features of the dataset for visualization
plt.figure(figsize=(12, 8))
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='coolwarm', alpha=0.6, edgecolor='w', s=100)
plt.title("Decision Tree Classifier Predictions")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# Implementing the bootstrapping algorithm
def bootstrap_sample(X, y):
    n_samples = X.shape[0]
    indices = np.random.choice(n_samples, size=n_samples, replace=True)
    return X[indices], y[indices]

# Training multiple Decision Tree Classifiers with bootstrapping
n_trees = 100
forest = []

for _ in range(n_trees):
    X_sample, y_sample = bootstrap_sample(X_train, y_train)
    tree = DecisionTreeClassifier(random_state=42)
    tree.fit(X_sample, y_sample)
    forest.append(tree)

# Making predictions
def forest_predict(forest, X):
    tree_preds = np.array([tree.predict(X) for tree in forest])
    return np.squeeze(np.apply_along_axis(lambda x: np.bincount(x, minlength=2).argmax(), arr=tree_preds, axis=0))

y_pred = forest_predict(forest, X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:\n", classification_report(y_test, y_pred))

# Plotting the first two features of the dataset for visualization
plt.figure(figsize=(12, 8))
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='coolwarm', alpha=0.6, edgecolor='w', s=100)
plt.title("Bootstrapped Decision Tree Classifier Predictions")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# For demonstration purposes, create a synthetic classification dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Convert to DataFrame for easier manipulation
feature_names = [f'Feature_{i}' for i in range(X.shape[1])]
df = pd.DataFrame(X, columns=feature_names)
df['Target'] = y

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df[feature_names], df['Target'], test_size=0.2, random_state=42)

# Train a Decision Tree classifier
decision_tree = DecisionTreeClassifier(random_state=42)
decision_tree.fit(X_train, y_train)

# Calculate the correlation matrix for the features
correlation_matrix = df[feature_names].corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix for Features')
plt.show()

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression

# Training the Logistic Regression model
log_reg = LogisticRegression(random_state=42, max_iter=1000)
log_reg.fit(X_train, y_train)

# Making predictions
y_pred = log_reg.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:\n", classification_report(y_test, y_pred))

# Plotting the first two features of the dataset for visualization
plt.figure(figsize=(12, 8))
plt.scatter(X_test.iloc[:, 0], X_test.iloc[:, 1], c=y_pred, cmap='coolwarm', alpha=0.6, edgecolor='w', s=100)
plt.title("Logistic Regression Classifier Predictions")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# For demonstration purposes, create a synthetic classification dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Convert to DataFrame for easier manipulation
feature_names = [f'Feature_{i}' for i in range(X.shape[1])]
df = pd.DataFrame(X, columns=feature_names)
df['Target'] = y

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df[feature_names], df['Target'], test_size=0.2, random_state=42)

# Train a Logistic Regression classifier
logistic_regression = LogisticRegression(max_iter=1000, random_state=42)
logistic_regression.fit(X_train, y_train)

# Calculate the correlation matrix for the features
correlation_matrix = df[feature_names].corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix for Features')
plt.show()

from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc

# Plotting the bar graph for accuracy
plt.figure(figsize=(10, 6))
plt.bar(['Logistic Regression'], [accuracy], color='green')
plt.ylabel('Accuracy Score')
plt.title('Accuracy of Logistic Regression Model')
plt.ylim(0, 1)
plt.show()

# Training the Logistic Regression model
log_reg = LogisticRegression(random_state=42, max_iter=1000)
log_reg.fit(X_train, y_train)

# Making predictions
y_pred = log_reg.predict(X_test)
y_pred_proba = log_reg.predict_proba(X_test)[:, 1]

# Plotting the histogram for predicted probabilities
plt.figure(figsize=(10, 6))
plt.hist(y_pred_proba, bins=10, color='blue', edgecolor='black', alpha=0.7)
plt.xlabel('Predicted Probability')
plt.ylabel('Frequency')
plt.title('Histogram of Predicted Probabilities')
plt.show()

# Plotting the first two features of the dataset for visualization
plt.figure(figsize=(12, 8))
plt.scatter(X_test.iloc[:, 0], X_test.iloc[:, 1], c=y_pred, cmap='coolwarm', alpha=0.6, edgecolor='w', s=100)
plt.title("Logistic Regression Classifier Predictions")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# For demonstration purposes, create a synthetic classification dataset
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Number of bootstrap samples
n_bootstrap = 100
accuracy_scores = []

for _ in range(n_bootstrap):
    # Create a bootstrap sample
    sample_indices = np.random.choice(len(X_train), size=len(X_train), replace=True)
    X_sampled = X_train[sample_indices]
    y_sampled = y_train[sample_indices]

    # Train a Logistic Regression model
    model = LogisticRegression(random_state=42, max_iter=1000)
    model.fit(X_sampled, y_sampled)

    # Evaluate the model on the original test set
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores.append(accuracy)

# Calculate and print the mean and standard deviation of accuracy scores
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)
print(f'Mean Accuracy: {mean_accuracy:.4f}')
print(f'Standard Deviation of Accuracy: {std_accuracy:.4f}')

# Convert accuracy scores to numpy array for easy manipulation
accuracy_scores = np.array(accuracy_scores)

# Plotting the histogram of accuracy scores
plt.figure(figsize=(10, 6))
plt.hist(accuracy_scores, bins=10, edgecolor='black', alpha=0.7)
plt.xlabel('Accuracy Score')
plt.ylabel('Frequency')
plt.title('Histogram of Accuracy Scores (Bootstrapped Logistic Regression)')
plt.grid(True)
plt.show()

# Number of bootstrap samples
n_bootstrap = 100
accuracy_scores = []

for _ in range(n_bootstrap):
    # Create a bootstrap sample
    sample_indices = np.random.choice(len(X_train), size=len(X_train), replace=True)
    X_sampled = X_train[sample_indices]
    y_sampled = y_train[sample_indices]

    # Train a Logistic Regression model
    model = LogisticRegression(random_state=42, max_iter=1000)
    model.fit(X_sampled, y_sampled)

    # Evaluate the model on the original test set
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores.append(accuracy)

# Convert accuracy scores to numpy array for easy manipulation
accuracy_scores = np.array(accuracy_scores)

# Plotting the graph of accuracy scores
plt.figure(figsize=(10, 6))
plt.plot(range(1, n_bootstrap + 1), accuracy_scores, marker='o', linestyle='-', color='b', alpha=0.7)
plt.xlabel('Bootstrap Sample')
plt.ylabel('Accuracy Score')
plt.title('Accuracy Scores of Bootstrapped Logistic Regression')
plt.grid(True)
plt.show()

# Calculate and print the mean and standard deviation of accuracy scores
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)
print(f'Mean Accuracy: {mean_accuracy:.4f}')
print(f'Standard Deviation of Accuracy: {std_accuracy:.4f}')

"""KNN Classification

"""

from sklearn.neighbors import KNeighborsClassifier

# For demonstration purposes, create a synthetic classification dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Training the k-Nearest Neighbors (k-NN) classifier
k = 5  # Choose the number of neighbors
knn = KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train, y_train)

# Making predictions
y_pred = knn.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:\n", classification_report(y_test, y_pred))

# Plotting the first two features of the dataset for visualization
plt.figure(figsize=(12, 8))
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='coolwarm', alpha=0.6, edgecolor='w', s=100)
plt.title("k-Nearest Neighbors (k-NN) Classifier Predictions")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# Number of bootstrap samples
n_bootstrap = 100
accuracy_scores = []

for _ in range(n_bootstrap):
    # Create a bootstrap sample
    sample_indices = np.random.choice(len(X_train), size=len(X_train), replace=True)
    X_sampled = X_train[sample_indices]
    y_sampled = y_train[sample_indices]

    # Train a k-Nearest Neighbors (k-NN) classifier
    k = 5  # Choose the number of neighbors
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_sampled, y_sampled)

    # Evaluate the model on the original test set
    y_pred = knn.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores.append(accuracy)

# Convert accuracy scores to numpy array for easy manipulation
accuracy_scores = np.array(accuracy_scores)

# Plotting the graph of accuracy scores
plt.figure(figsize=(10, 6))
plt.plot(range(1, n_bootstrap + 1), accuracy_scores, marker='o', linestyle='-', color='b', alpha=0.7)
plt.xlabel('Bootstrap Sample')
plt.ylabel('Accuracy Score')
plt.title('Accuracy Scores of Bootstrapped k-NN Classification')
plt.grid(True)
plt.show()

# Calculate and print the mean and standard deviation of accuracy scores
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)
print(f'Mean Accuracy: {mean_accuracy:.4f}')
print(f'Standard Deviation of Accuracy: {std_accuracy:.4f}')

# Plotting the bar graph of mean accuracy
mean_accuracy = np.mean(accuracy_scores)
plt.figure(figsize=(8, 6))
plt.bar(['k-NN'], [mean_accuracy], color='blue', alpha=0.7)
plt.ylim(0, 1)
plt.ylabel('Accuracy')
plt.title('Mean Accuracy of Bootstrapped k-NN Classification')
plt.show()

# Plotting the histogram of accuracy scores
plt.figure(figsize=(10, 6))
plt.hist(accuracy_scores, bins=10,color='pink', edgecolor='black', alpha=0.7)
plt.xlabel('Accuracy Score')
plt.ylabel('Frequency')
plt.title('Histogram of Accuracy Scores (Bootstrapped k-NN Classification)')
plt.grid(True)
plt.show()

# For demonstration purposes, create a synthetic classification dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Convert to DataFrame for easier manipulation
feature_names = [f'Feature_{i}' for i in range(X.shape[1])]
df = pd.DataFrame(X, columns=feature_names)
df['Target'] = y

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df[feature_names], df['Target'], test_size=0.2, random_state=42)

# Train a KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Calculate the correlation matrix for the features
correlation_matrix = df[feature_names].corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix for Features')
plt.show()

"""Naive Bayes Classification"""

from sklearn.naive_bayes import GaussianNB

# Training the Naive Bayes classifier (Gaussian Naive Bayes)
nb = GaussianNB()
nb.fit(X_train, y_train)

# Making predictions
y_pred = nb.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Classification Report:\n", classification_report(y_test, y_pred))

# Plotting the first two features of the dataset for visualization
plt.figure(figsize=(12, 8))
plt.scatter(X_test.iloc[:, 0], X_test.iloc[:, 1], c=y_pred, cmap='coolwarm', alpha=0.6, edgecolor='w', s=100)
plt.title("Naive Bayes Classifier Predictions")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# For demonstration purposes, create a synthetic classification dataset
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)

# Convert to DataFrame for easier manipulation
feature_names = [f'Feature_{i}' for i in range(X.shape[1])]
df = pd.DataFrame(X, columns=feature_names)
df['Target'] = y

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df[feature_names], df['Target'], test_size=0.2, random_state=42)

# Train a Naive Bayes classifier
naive_bayes = GaussianNB()
naive_bayes.fit(X_train, y_train)

# Calculate the correlation matrix for the features
correlation_matrix = df[feature_names].corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix for Features')
plt.show()

# Plotting the bar graph for accuracy
plt.figure(figsize=(8, 6))
plt.bar(['Naive Bayes'], [accuracy], color='blue', alpha=0.7)
plt.ylim(0, 1)
plt.ylabel('Accuracy')
plt.title('Accuracy of Naive Bayes Classifier')
plt.show()

# Plotting the histogram of predicted probabilities
y_pred_proba = nb.predict_proba(X_test)[:, 1]
plt.figure(figsize=(10, 6))
plt.hist(y_pred_proba, bins=10, color='pink', edgecolor='black', alpha=0.7)
plt.xlabel('Predicted Probability')
plt.ylabel('Frequency')
plt.title('Histogram of Predicted Probabilities (Naive Bayes)')
plt.grid(True)
plt.show()

# Number of bootstrap samples
n_bootstrap = 100
accuracy_scores = []

for _ in range(n_bootstrap):
    # Create a bootstrap sample
    sample_indices = np.random.choice(len(X_train), size=len(X_train), replace=True)
    X_sampled = X_train.iloc[sample_indices]
    y_sampled = y_train.iloc[sample_indices]

    # Train a Naive Bayes classifier (Gaussian Naive Bayes)
    nb = GaussianNB()
    nb.fit(X_sampled, y_sampled)

    # Evaluate the model on the original test set
    y_pred = nb.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores.append(accuracy)

# Convert accuracy scores to numpy array for easy manipulation
accuracy_scores = np.array(accuracy_scores)

# Plotting the graph of accuracy scores
plt.figure(figsize=(10, 6))
plt.plot(range(1, n_bootstrap + 1), accuracy_scores, marker='o', linestyle='-', color='b', alpha=0.7)
plt.xlabel('Bootstrap Sample')
plt.ylabel('Accuracy Score')
plt.title('Accuracy Scores of Bootstrapped Naive Bayes Classification')
plt.grid(True)
plt.show()

# Calculate and print the mean and standard deviation of accuracy scores
mean_accuracy = np.mean(accuracy_scores)
std_accuracy = np.std(accuracy_scores)
print(f'Mean Accuracy: {mean_accuracy:.4f}')
print(f'Standard Deviation of Accuracy: {std_accuracy:.4f}')

"""Clustering"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.cluster import DBSCAN

# Generate synthetic data (moon-shaped clusters)
X, _ = make_moons(n_samples=1000, noise=0.1, random_state=42)

# Visualize the dataset
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')
plt.title('Synthetic Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# Applying DBSCAN clustering
dbscan = DBSCAN(eps=0.2, min_samples=5)
labels = dbscan.fit_predict(X)

# Visualize the clustering results
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')
plt.title('DBSCAN Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

# Generate synthetic data
X, _ = make_blobs(n_samples=1000, centers=4, cluster_std=1.0, random_state=42)

# Visualize the dataset
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')
plt.title('Synthetic Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# Generate synthetic data
X, _ = make_blobs(n_samples=1000, centers=4, cluster_std=1.0, random_state=42)

# Convert to DataFrame for easier manipulation
feature_names = [f'Feature_{i}' for i in range(X.shape[1])]
df = pd.DataFrame(X, columns=feature_names)

# Applying DBSCAN clustering
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X)

# Add the cluster labels to the DataFrame
df['Cluster'] = labels

# Calculate the correlation matrix for the features
correlation_matrix = df[feature_names].corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix for Features')
plt.show()

# Applying K-Means clustering
k = 4  # Number of clusters
kmeans = KMeans(n_clusters=k, random_state=42)
labels = kmeans.fit_predict(X)

# Visualize the clustering results
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')
plt.title('K-Means Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

from sklearn.mixture import GaussianMixture

# Generate synthetic data
X, _ = make_blobs(n_samples=1000, centers=4, cluster_std=1.0, random_state=42)

# Visualize the dataset
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')
plt.title('Synthetic Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# Applying KMeans clustering
kmeans = KMeans(n_clusters=4, random_state=42)
labels = kmeans.fit_predict(X)

# Add the cluster labels to the DataFrame
df['Cluster'] = labels

# Calculate the correlation matrix for the features
correlation_matrix = df[feature_names].corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix for Features')
plt.show()

# Applying Gaussian Mixture Model (GMM) clustering
k = 4  # Number of clusters
gmm = GaussianMixture(n_components=k, random_state=42)
labels = gmm.fit_predict(X)

# Visualize the clustering results
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')
plt.title('Gaussian Mixture Model (GMM) Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

from sklearn.datasets import make_blobs
from scipy.cluster.hierarchy import dendrogram, linkage

# Generate synthetic data
X, _ = make_blobs(n_samples=1000, centers=4, cluster_std=1.0, random_state=42)

# Visualize the dataset
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')
plt.title('Synthetic Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# Convert to DataFrame for easier manipulation
feature_names = [f'Feature_{i}' for i in range(X.shape[1])]
df = pd.DataFrame(X, columns=feature_names)

# Applying Gaussian Mixture Model clustering
gmm = GaussianMixture(n_components=4, random_state=42)
labels = gmm.fit_predict(X)

# Add the cluster labels to the DataFrame
df['Cluster'] = labels

# Calculate the correlation matrix for the features
correlation_matrix = df[feature_names].corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix for Features')
plt.show()

# Applying Hierarchical Clustering (Agglomerative)
Z = linkage(X, method='ward')  # Perform hierarchical/agglomerative clustering
plt.figure(figsize=(12, 8))
dendrogram(Z)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.show()

from sklearn.datasets import make_blobs
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.preprocessing import StandardScaler

# Generate synthetic data
X, _ = make_blobs(n_samples=1000, centers=4, cluster_std=1.0, random_state=42)

# Convert to DataFrame for easier manipulation
feature_names = [f'Feature_{i}' for i in range(X.shape[1])]
df = pd.DataFrame(X, columns=feature_names)

# Standardizing the features (important for hierarchical clustering)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Applying hierarchical clustering
Z = linkage(X_scaled, method='ward')

# Assign cluster labels
labels = fcluster(Z, t=4, criterion='maxclust')

# Add the cluster labels to the DataFrame
df['Cluster'] = labels

# Calculate the correlation matrix for the features
correlation_matrix = df[feature_names].corr()

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix for Features')
plt.show()